{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Transformers for GPUs with Optimum\n",
    "\n",
    "In this session, you will learn how to optimize Hugging Face Transformers models for GPUs using Optimum. The session will show you how to convert you weights to fp16 weights and optimize a DistilBERT model using [Hugging Face Optimum](https://huggingface.co/docs/optimum/index) and [ONNX Runtime](https://onnxruntime.ai/). Hugging Face Optimum is an extension of ðŸ¤— Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware. We are going to optimize a DistilBERT model for Question Answering, which was fine-tuned on the SQuAD dataset to decrease the latency from 7ms to 3ms for a sequence lenght of 128.\n",
    "\n",
    "_Note: int8 quantization  is currently only supported for CPUs. We plan to add support for in the near future using TensorRT._\n",
    "\n",
    "By the end of this session, you will know how GPU optimization with Hugging Face Optimum can result in significant increase in model latency and througput while keeping  100% of the full-precision model. \n",
    "\n",
    "You will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Convert a Hugging Face `Transformers` model to ONNX for inference\n",
    "3. Optimize model for GPU using `ORTOptimizer`\n",
    "4. Evaluate the performance and speed\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "_This tutorial was created and run on an g4dn.xlarge AWS EC2 Instance including a NVIDIA T4._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Optimum, along with  Evaluate and some other libraries. Running the following cell will install all the required packages for us including Transformers, PyTorch, and ONNX Runtime utilities:\n",
    "\n",
    "_Note: You need a machine with a GPU and CUDA installed. You can check this by running `nvidia-smi` in your terminal. If you have a correct environment you should statistics abour your GPU._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"optimum[onnxruntime-gpu]==1.3.0\" --upgrade\n",
    "%pip install git+https://github.com/huggingface/optimum.git --user\n",
    "%pip install  onnx datasets datasets --upgrade --user\n",
    "%pip install -i https://test.pypi.org/simple/ \"ort-nightly-gpu==1.12.0.dev20220711001\" # needed since 1.11.1 has a bug for gpu optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start. Lets make sure we have the `CUDAExecutionProvider` for ONNX Runtime available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import get_available_providers, get_device\n",
    "import onnxruntime \n",
    "\n",
    "# check available providers\n",
    "assert 'CUDAExecutionProvider' in get_available_providers(), \"ONNX Runtime GPU provider not found. Make sure onnxruntime-gpu is installed and onnxruntime is uninstalled.\"\n",
    "assert \"GPU\" == get_device()\n",
    "\n",
    "# asser version due to bug in 1.11.1\n",
    "assert onnxruntime.__version__ > \"1.11.1\", \"you need a newer version of ONNX Runtime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want to run inference on a CPU, you can install ðŸ¤— Optimum with `pip install optimum[onnxruntime]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert a Hugging Face `Transformers` model to ONNX for inference\n",
    "\n",
    "Before we can start optimizing our model we need to convert our vanilla `transformers` model to the `onnx` format. To do this we will use the new [ORTModelForQuestionAnswering](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForQuestionAnswering) class calling the `from_pretrained()` method with the `from_transformers` attribute. The model we are using is the [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad) a fine-tuned DistilBERT-based model on the SQuAD dataset achieving an F1 score of `87.1` and as the feature (task) `question-answering`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:00<00:00, 300kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00<00:00, 21.5kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:00<00:00, 436kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208k/208k [00:00<00:00, 57.7MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426k/426k [00:00<00:00, 74.2MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249M/249M [00:02<00:00, 102MB/s] \n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:214: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_id=\"distilbert-base-cased-distilled-squad\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into the optimization of the model lets first evaluate the current performance of the model. Therefore we can use `pipeline()` function from ðŸ¤— Transformers. Meaning we will measure the end-to-end latency including the pre- and post-processing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\" \n",
    "question=\"As what is Philipp working?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we prepared our payload we can create the inference `pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline is loaded on device cuda:0\n",
      "{'score': 0.6575328707695007, 'start': 88, 'end': 102, 'answer': 'Technical Lead'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vanilla_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "print(f\"pipeline is loaded on device {vanilla_qa.model.device}\")\n",
    "print(vanilla_qa(question=question,context=context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are seeing a `CreateExecutionProviderInstance` error you are not having a compatible `cuda` version installed. Check the [documentation](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html), which cuda version you need. \n",
    "\n",
    "If you want to learn more about exporting transformers model check-out [Convert Transformers to ONNX with Hugging Face Optimum](https://www.philschmid.de/convert-transformers-to-onnx) blog post\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimize model for GPU using `ORTOptimizer` \n",
    "\n",
    "The [ORTOptimizer](https://huggingface.co/docs/optimum/onnxruntime/optimization#optimum.onnxruntime.ORTOptimizer) allows you to apply ONNX Runtime optimization on our Transformers models. In addition to the `ORTOptimizer` Optimum offers a [OptimizationConfig](https://huggingface.co/docs/optimum/onnxruntime/configuration#optimum.onnxruntime.configuration.OptimizationConfig) a configuration class handling all the ONNX Runtime optimization parameters. \n",
    "There are several technique to optimize our model for GPUs including graph optimizations and converting our model weights from `fp32` to `fp16`. \n",
    "\n",
    "Graph optimizations are essentially graph-level transformations, ranging from small graph simplifications and node eliminations to more complex node fusions and layout optimizations. \n",
    "Examples of graph optimizations include:\n",
    "* **Constant folding**: evaluate constant expressions at compile time instead of runtime\n",
    "* **Redundant node elimination**: remove redundant nodes without changing graph structure\n",
    "* **Operator fusion**: merge one node (i.e. operator) into another so they can be executed together\n",
    "\n",
    "\n",
    "![operator fusion](./assets/operator_fusion.png)\n",
    "\n",
    "If you want to learn more about graph optimization you take a look at the [ONNX Runtime documentation](https://onnxruntime.ai/docs/performance/graph-optimizations.html).\n",
    "\n",
    "To achieve best performance we will apply the following optimizations parameter in our `OptimizationConfig`:\n",
    "* `optimization_level=99`: to enable all the optimizations. _Note: Switching Hardware after optimization can lead to issues._\n",
    "* `optimize_for_gpu=True`: to enable GPU optimizations. \n",
    "* `fp16=True`: to convert model computation from `fp32` to `fp16`. _Note: Only for V100 and T4 or newer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 08:24:21.953075432 [W:onnxruntime:, inference_session.cc:1488 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-optimized.onnx')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model_id, feature=model.pipeline_task)\n",
    "optimization_config = OptimizationConfig(optimization_level=99,\n",
    "                                         optimize_for_gpu=True,\n",
    "                                         fp16=True\n",
    "                                         )\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test performance we can use the ORTModelForSequenceClassification class again and provide an additional `file_name` parameter to load our optimized model. _(This also works for models available on the hub)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6544352173805237, 'start': 88, 'end': 102, 'answer': 'Technical Lead'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# load optimized model\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model-optimized.onnx\")\n",
    "\n",
    "# create optimized pipeline\n",
    "optimized_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "print(optimized_qa(question=question,context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the performance and speed\n",
    "\n",
    "As the last step, we want to take a detailed look at the performance and accuracy of our model. Applying optimization techniques, like graph optimizations or mixed-precision not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off. \n",
    "\n",
    "Let's evaluate our models. Our transformers model [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad) was fine-tuned on the SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "trfs_qa = pipeline(\"question-answering\", model=model_id, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 690.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric,load_dataset\n",
    "\n",
    "metric = load_metric(\"squad_v2\")\n",
    "metric = load_metric(\"squad\")\n",
    "eval_dataset = load_dataset(\"squad\")[\"validation\"]\n",
    "\n",
    "# creating a subset for faster evaluation\n",
    "# COMMENT IN to run evaluation on a subset of the dataset\n",
    "# eval_dataset = eval_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) function of [datasets](https://huggingface.co/docs/datasets/index) to iterate over the validation set of `squad_v2` and run prediction for each data point. Therefore we write a `evaluate` helper method which uses our pipelines and applies some transformation to work with the [squad v2 metric.](https://huggingface.co/metrics/squad_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [02:08<00:00, 82.04ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model: f1=86.84859514665654%\n",
      "optimized model: f1=86.8536859246896%\n",
      "The optimized model achieves 100.00% accuracy of the fp32 model\n"
     ]
    }
   ],
   "source": [
    "def evaluate(example):\n",
    "  default = vanilla_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  optimized = optimized_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer']},\n",
    "      'optimized': {'id': example['id'],'prediction_text': optimized['answer']},\n",
    "      }\n",
    "\n",
    "result = eval_dataset.map(evaluate)\n",
    "\n",
    "default_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "optimized = metric.compute(predictions=result[\"optimized\"], references=result[\"reference\"])\n",
    "\n",
    "print(f\"vanilla model: f1={default_acc['f1']}%\")\n",
    "print(f\"optimized model: f1={optimized['f1']}%\")\n",
    "print(f\"The optimized model achieves {round(optimized['f1']/default_acc['f1'],2)*100:.2f}% accuracy of the fp32 model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's test the performance (latency) of our optimized model. We are going to use a payload with a sequence length of 128 for the benchmark. To keep it simple, we are going to use a python loop and calculate the avg,mean & p95 latency for our vanilla model and for the optimized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: P95 latency (ms) - 7.784631400227273; Average latency (ms) - 6.87 +\\- 1.20;\n",
      "Optimized model: P95 latency (ms) - 3.392388850079442; Average latency (ms) - 3.32 +\\- 0.03;\n",
      "Improvement through optimization: 2.29x\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\" \n",
    "question=\"As what is Philipp working?\" \n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question,context=context)\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question,context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "vanilla_model=measure_latency(vanilla_qa)\n",
    "optimized_model=measure_latency(optimized_qa)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Optimized model: {optimized_model[0]}\")\n",
    "print(f\"Improvement through optimization: {round(vanilla_model[1]/optimized_model[1],2)}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to accelerate our model latency from 7.8ms to 3.4ms or 2.3x while keeping 100.00% of the accuracy. \n",
    "\n",
    "![performance](assets/performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We successfully optimized our vanilla Transformers model with Hugging Face Optimum and managed to accelerate our model latency from 7.8ms to 3.4ms or 2.3x while keeping 100.00% of the accuracy. \n",
    "\n",
    "But I have to say that this isn't a plug and play process you can transfer to any Transformers model, task or dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
